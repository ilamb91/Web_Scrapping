{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ed785a-33c0-46d4-8408-26680394050a",
   "metadata": {},
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba53f0-b92e-488b-8eaa-b9580297d695",
   "metadata": {},
   "source": [
    "- Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "\n",
    "- Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.\n",
    "\n",
    "- Web Scrapers can extract all the data on particular sites or the specific data that a user wants. Ideally, it’s best if you specify the data you want so that the web scraper only extracts that data quickly. For example, you might want to scrape an Amazon page for the types of juicers available, but you might only want the data about the models of different juicers and not the customer reviews. \n",
    "\n",
    "- Web Scraping has multiple applications across various industries. Let’s check out some of these now!\n",
    "\n",
    "1. Price Monitoring\n",
    "- Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research\n",
    "- Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring \n",
    "- Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!\n",
    "\n",
    "4. Sentiment Analysis\n",
    "- If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.\n",
    "\n",
    "5. Email Marketing\n",
    "- Companies can also use Web scraping for email marketing. They can collect Email ID’s from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email ID’s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa02883-0121-49cb-b79c-07b08620c78a",
   "metadata": {},
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf0bf2-e54b-4a73-884d-d3620cda9342",
   "metadata": {},
   "source": [
    "A3. Web Scrapers can be divided on the basis of many different criteria, including Self-built or Pre-built Web Scrapers, Browser extension or Software Web Scrapers, and Cloud or Local Web Scrapers.\n",
    "\n",
    "- We can have Self-built Web Scrapers but that requires advanced knowledge of programming. And if we want more features in your Web Scraper, then we need even more knowledge. On the other hand, pre-built Web Scrapers are previously created scrapers that we can download and run easily. These also have more advanced options that we can customize.\n",
    "\n",
    "- Browser extensions Web Scrapers are extensions that can be added to your browser. These are easy to run as they are integrated with your browser, but at the same time, they are also limited because of this. Any advanced features that are outside the scope of your browser are impossible to run on Browser extension Web Scrapers. But Software Web Scrapers don’t have these limitations as they can be downloaded and installed on your computer. These are more complex than Browser web scrapers, but they also have advanced features that are not limited by the scope of your browser.\n",
    "\n",
    "- Cloud Web Scrapers run on the cloud, which is an off-site server mostly provided by the company that we buy the scraper from. These allow your computer to focus on other tasks as the computer resources are not required to scrape data from websites. Local Web Scrapers, on the other hand, run on your computer using local resources. So, if the Web scrapers require more CPU or RAM, then your computer will become slow and not be able to perform other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd770588-5b72-42e7-af2b-9b58f817b579",
   "metadata": {},
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72fd94-8a91-4a81-ba83-d5d0471a3a1c",
   "metadata": {},
   "source": [
    "A3. The Beautiful Soup is a python library which is named after a Lewis Carroll poem of the same name in “Alice’s Adventures in the Wonderland”. Beautiful Soup is a python package and as the name suggests, parses the unwanted data and helps to organize and format the messy web data by fixing bad HTML and present to us in an easily-traversible XML structures.\n",
    "# Uses of Beautiful Soup\n",
    "- The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working.\n",
    "# Features of Beautiful Soup\n",
    "- Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
    "- Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "- Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, which allows​ us to try out different parsing strategies or trade speed for flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da879b8-0eef-43c6-a124-fe0ff3b84e01",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240df98f-2f38-4b82-a623-5051c7e0e5bf",
   "metadata": {},
   "source": [
    "A4. Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape. The first line imports the Flask class and the render_template method from the flask library.\n",
    "- Flask is an API of Python that allows us to build up web-applications. It was developed by Armin Ronacher. Flask’s framework is more explicit than Django’s framework and is also easier to learn because it has less base code to implement a simple web-Application. A Web-Application Framework or Web Framework is the collection of modules and libraries that helps the developer to write applications without writing the low-level codes such as protocols, thread management, etc. Flask is based on WSGI(Web Server Gateway Interface) toolkit and Jinja2 template engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad2138-a1fa-4511-ba00-9b11136f2bf7",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0accc2-105f-485b-97c7-ef6bee28855a",
   "metadata": {},
   "source": [
    "A5. AWS (Amazon Web Services) is the largest cloud computing platform with over 200+ featured resources. It is a platform that provides a pay-as-you-go service. Gone are the days when you need to install servers and look after maintenance and cost of them. AWS offers a “pay-as-you-go” feature where we just have to pay for the services you use and also the time period you use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb510ac-39a1-447a-a616-f559e797877c",
   "metadata": {},
   "source": [
    "1. Amazon EC2 (Elastic Cloud Compute)\n",
    "- Amazon EC2 is the fastest cloud computing service provided by AWS. It offers virtual, secure, reliable, and resizable servers for any workload. Through this service, it becomes easy for developers to access resources and also facilitates web-scale cloud computing. This comes with the best suitable processors, networking facilities, and storage systems. Developers can quickly and dynamically scale capacities as per business needs. It has over 500 instances and you can also choose the latest processor, operating system, storage, and networking to help you choose according to the needs of the business. Also, with Amazon EC2, you only have to pay for what you use, and also as per the time period, scale with amazon EC2 auto-scaling has optimal storage and can optimize CPU configurations.\n",
    "\n",
    "2. Amazon IAM (Identity and Access Management)\n",
    "- Amazon IAM (Identity and Access Management) allows users to securely access and manage resources. To achieve complete access to the tools and resources provided by AWS, AWS IAM is the best AWS service. It gives you the right to have control over who has authorization (signed in) and authentication (has permissions) access to the resources. It comes with attribute-based access control which helps you to create separate permissions on the basis of the user’s attributes such as job role, department, etc. Through this, you can allow or deny access given to users. AWS IAM has complete access or is a central manager for refining permissions across AWS. He/She handles who can access what.\n",
    "\n",
    "3. Amazon VPC (Virtual Private Cloud)\n",
    "- Another AWS service is Amazon VPC (Virtual Private Cloud) which is an isolated cloud resource, it enables you to set up an isolated section where you can deploy AWS resources at scale in a virtual environment. This service is responsible to control the virtual networking environment such as resource placement, security, and connectivity. Security can be improved by applying rules for outbound and inbound connections. Also, it detects anomalies in the patterns, troubleshoots network connections, prevents data leakage, and handles configuration issues. Using VPC, you get complete access to control the environment, such as choosing IP address, subset creation, and route table arrangement. \n",
    "\n",
    "4. Amazon Elastic Beanstalk\n",
    "- Amazon Elastic Beanstalk is an AWS service used for deployment and scaling web applications developed using Java, PHP, Python, Docker, etc. It supports running and managing web applications. You just need to upload your code and the deployment part is handled by Elastic Beanstalk (from capacity provisioning, load balancing, and auto-scaling to application health monitoring). It is the best service for developers since it takes care of the servers, load balancers, and firewalls. Also, you can have control over AWS assets and the other resources required for the application. You get the benefit of paying for what you use, thus maintaining cost-effectiveness.  \n",
    "\n",
    "5. AWS CodePipeline\n",
    "- AWS CodePipeline is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. CodePipeline automates the steps required to release your software changes continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae79c7-b38b-4414-b117-d0e2e70c5117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
